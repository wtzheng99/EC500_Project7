{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0964589a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from modules import SAB, PMA\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "num_classes = 4\n",
    "\n",
    "\n",
    "class SmallDeepSet(nn.Module):\n",
    "    def __init__(self, pool=\"max\"):\n",
    "        super().__init__()\n",
    "        self.enc = nn.Sequential(\n",
    "            nn.Linear(in_features=2048, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=64),\n",
    "        )\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.Linear(in_features=64, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=num_classes),\n",
    "        )\n",
    "        self.pool = pool\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.enc(x)\n",
    "        if self.pool == \"max\":\n",
    "            x = x.max(dim=1)[0]\n",
    "        elif self.pool == \"mean\":\n",
    "            x = x.mean(dim=1)\n",
    "        elif self.pool == \"sum\":\n",
    "            x = x.sum(dim=1)\n",
    "        x = self.dec(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SmallSetTransformer(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super().__init__()\n",
    "        self.enc = nn.Sequential(\n",
    "            SAB(dim_in=2048, dim_out=64, num_heads=4),\n",
    "            SAB(dim_in=64, dim_out=64, num_heads=4),\n",
    "        )\n",
    "        self.dec = nn.Sequential(\n",
    "            PMA(dim=64, num_heads=4, num_seeds=1),\n",
    "            nn.Linear(in_features=64, out_features=num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.enc(x)\n",
    "        x = self.dec(x)\n",
    "        return x.squeeze(-1)\n",
    "\n",
    "\n",
    "\n",
    "# Dataset class definition\n",
    "class SetDataset(Dataset):\n",
    "    def __init__(self, data_folder, annotations_file, mode='train'):\n",
    "        self.data_folder = data_folder\n",
    "        self.annotations = pd.read_csv(annotations_file)\n",
    "        self.mode = mode\n",
    "        self.annotations['path'] = self.annotations['filename'] + '_' + self.annotations['x_y'] + '.pt'\n",
    "        self.annotations.set_index('path', inplace=True)\n",
    "        self.data_files = [f for f in os.listdir(data_folder) if f.endswith('.pt')]\n",
    "        if mode == 'train':\n",
    "            self.data_files = [f for f in self.data_files if not any(x in f for x in ['case3', 'case4', 'control3', 'control4'])]\n",
    "        else:\n",
    "            self.data_files = [f for f in self.data_files if any(x in f for x in ['case3', 'case4', 'control3', 'control4'])]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        file_path = self.data_files[index]\n",
    "        data = torch.load(os.path.join(self.data_folder, file_path))\n",
    "        label = self.annotations.loc[file_path, 'level']\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        return data, label\n",
    "\n",
    "# # Collate function to handle variable-sized data\n",
    "# def collate_fn(batch):\n",
    "#     xs, ys = zip(*batch)\n",
    "#     return list(xs), torch.tensor(ys, dtype=torch.long)\n",
    "def collate_fn(batch):\n",
    "    xs, ys = zip(*batch)\n",
    "    # Pad the sequences so they all have the same length\n",
    "    xs = pad_sequence(xs, batch_first=True)  # Pads with zero by default\n",
    "    ys = torch.tensor(ys, dtype=torch.long)\n",
    "    return xs, ys\n",
    "\n",
    "\n",
    "\n",
    "def train(model, data_loader, epochs, save_path):\n",
    "    # Move model to GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)  # Using CrossEntropyLoss as it is common for classification tasks\n",
    "    losses = []\n",
    "\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for data, label in data_loader:\n",
    "            data = data.to(device)\n",
    "            label = label.to(device)\n",
    "            output = model(data)\n",
    "            output = output.squeeze(1)  # This changes shape from [20, 1, 4] to [20, 4]\n",
    "#             print(output)\n",
    "#             print(label)\n",
    "#             print(\"-\"*60)\n",
    "            loss = criterion(output, label)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}')  # Print loss for the epoch\n",
    "\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    return losses\n",
    "\n",
    "def evaluate(model, data_loader, load_path):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Load the saved weights\n",
    "    model.load_state_dict(torch.load(load_path))\n",
    "\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, label in data_loader:\n",
    "            data = data.to(device)\n",
    "            label = label.to(device)\n",
    "            output = model(data)\n",
    "            output = output.squeeze(1)\n",
    "            loss = criterion(output, label)\n",
    "            total_loss += loss.item()\n",
    "            predictions = output.argmax(dim=1, keepdim=True)\n",
    "            total_correct += predictions.eq(label.view_as(predictions)).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = total_correct / len(data_loader.dataset)\n",
    "    print(f'Test Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "data_folder = '/projectnb/ec500kb/projects/project7/GTEx/annotated_patches/resnet_features' # the path for the stored features\n",
    "annotations_file = '/projectnb/ec500kb/projects/project7/GTEx/annotated_patches/annotations.csv'\n",
    "train_dataset = SetDataset(data_folder, annotations_file, mode='train')\n",
    "train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "test_dataset = SetDataset(data_folder, annotations_file, mode='test')\n",
    "test_loader = DataLoader(test_dataset, batch_size=20, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7df1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a Set Transformer model defined as 'model'\n",
    "model = SmallSetTransformer()\n",
    "\n",
    "train_path = '/projectnb/ec500kb/projects/project7/set_transformer/model_weights.pth'\n",
    "evaluate_path = '/projectnb/ec500kb/projects/project7/set_transformer/model_weights.pth'\n",
    "\n",
    "train(model, train_loader, epochs=100, save_path=train_path)\n",
    "evaluate(model, test_loader, load_path=evaluate_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "30fe5c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.3716249465942383\n",
      "Epoch 2, Loss: 1.2860463857650757\n",
      "Epoch 3, Loss: 1.4683560132980347\n",
      "Epoch 4, Loss: 1.3566828966140747\n",
      "Epoch 5, Loss: 1.1399704217910767\n",
      "Epoch 6, Loss: 1.1359277963638306\n",
      "Epoch 7, Loss: 1.4350744485855103\n",
      "Epoch 8, Loss: 1.026046872138977\n",
      "Epoch 9, Loss: 1.3894871473312378\n",
      "Epoch 10, Loss: 0.9681815505027771\n",
      "Epoch 11, Loss: 1.458438515663147\n",
      "Epoch 12, Loss: 1.2430943250656128\n",
      "Epoch 13, Loss: 1.1677699089050293\n",
      "Epoch 14, Loss: 0.9662911295890808\n",
      "Epoch 15, Loss: 0.8242725729942322\n",
      "Epoch 16, Loss: 1.42889404296875\n",
      "Epoch 17, Loss: 1.0772634744644165\n",
      "Epoch 18, Loss: 0.9825153946876526\n",
      "Epoch 19, Loss: 1.156739354133606\n",
      "Epoch 20, Loss: 1.42207670211792\n",
      "Epoch 21, Loss: 1.1884562969207764\n",
      "Epoch 22, Loss: 1.0731812715530396\n",
      "Epoch 23, Loss: 1.3973110914230347\n",
      "Epoch 24, Loss: 1.1313308477401733\n",
      "Epoch 25, Loss: 1.0063930749893188\n",
      "Epoch 26, Loss: 1.1348589658737183\n",
      "Epoch 27, Loss: 1.4415658712387085\n",
      "Epoch 28, Loss: 1.5732933282852173\n",
      "Epoch 29, Loss: 0.9724490642547607\n",
      "Epoch 30, Loss: 0.9391624927520752\n",
      "Epoch 31, Loss: 1.2410004138946533\n",
      "Epoch 32, Loss: 0.9806376099586487\n",
      "Epoch 33, Loss: 1.7231627702713013\n",
      "Epoch 34, Loss: 1.0749869346618652\n",
      "Epoch 35, Loss: 1.0737801790237427\n",
      "Epoch 36, Loss: 1.1812692880630493\n",
      "Epoch 37, Loss: 1.3221083879470825\n",
      "Epoch 38, Loss: 0.9337067604064941\n",
      "Epoch 39, Loss: 1.1691795587539673\n",
      "Epoch 40, Loss: 1.2436128854751587\n",
      "Epoch 41, Loss: 1.0118556022644043\n",
      "Epoch 42, Loss: 1.728892207145691\n",
      "Epoch 43, Loss: 0.7385680675506592\n",
      "Epoch 44, Loss: 1.167932152748108\n",
      "Epoch 45, Loss: 0.7207505702972412\n",
      "Epoch 46, Loss: 0.8358385562896729\n",
      "Epoch 47, Loss: 0.8871739506721497\n",
      "Epoch 48, Loss: 1.5366020202636719\n",
      "Epoch 49, Loss: 1.3715609312057495\n",
      "Epoch 50, Loss: 1.1862287521362305\n",
      "Epoch 51, Loss: 0.8895409107208252\n",
      "Epoch 52, Loss: 0.8160752654075623\n",
      "Epoch 53, Loss: 1.2024294137954712\n",
      "Epoch 54, Loss: 1.0534186363220215\n",
      "Epoch 55, Loss: 0.9189556241035461\n",
      "Epoch 56, Loss: 1.0892614126205444\n",
      "Epoch 57, Loss: 1.2567070722579956\n",
      "Epoch 58, Loss: 0.9892264008522034\n",
      "Epoch 59, Loss: 1.0597105026245117\n",
      "Epoch 60, Loss: 1.1466333866119385\n",
      "Epoch 61, Loss: 1.2131372690200806\n",
      "Epoch 62, Loss: 0.6762083172798157\n",
      "Epoch 63, Loss: 1.1620358228683472\n",
      "Epoch 64, Loss: 0.5142824053764343\n",
      "Epoch 65, Loss: 0.7208145260810852\n",
      "Epoch 66, Loss: 0.9746986031532288\n",
      "Epoch 67, Loss: 0.9341561198234558\n",
      "Epoch 68, Loss: 1.3249591588974\n",
      "Epoch 69, Loss: 1.5894941091537476\n",
      "Epoch 70, Loss: 0.5807027220726013\n",
      "Epoch 71, Loss: 1.3346920013427734\n",
      "Epoch 72, Loss: 0.6779458522796631\n",
      "Epoch 73, Loss: 1.0310237407684326\n",
      "Epoch 74, Loss: 1.1066421270370483\n",
      "Epoch 75, Loss: 1.1793030500411987\n",
      "Epoch 76, Loss: 0.5551242828369141\n",
      "Epoch 77, Loss: 1.020298957824707\n",
      "Epoch 78, Loss: 0.6884453296661377\n",
      "Epoch 79, Loss: 1.1470613479614258\n",
      "Epoch 80, Loss: 1.408366322517395\n",
      "Epoch 81, Loss: 0.9474316239356995\n",
      "Epoch 82, Loss: 0.7274854183197021\n",
      "Epoch 83, Loss: 1.1996742486953735\n",
      "Epoch 84, Loss: 1.135233998298645\n",
      "Epoch 85, Loss: 0.7173254489898682\n",
      "Epoch 86, Loss: 0.7346081733703613\n",
      "Epoch 87, Loss: 0.7988879680633545\n",
      "Epoch 88, Loss: 0.5556371808052063\n",
      "Epoch 89, Loss: 0.619583010673523\n",
      "Epoch 90, Loss: 0.5861374735832214\n",
      "Epoch 91, Loss: 0.5964847207069397\n",
      "Epoch 92, Loss: 0.5536152720451355\n",
      "Epoch 93, Loss: 1.217827558517456\n",
      "Epoch 94, Loss: 0.8679559230804443\n",
      "Epoch 95, Loss: 0.6685650944709778\n",
      "Epoch 96, Loss: 0.9195612072944641\n",
      "Epoch 97, Loss: 0.7873384952545166\n",
      "Epoch 98, Loss: 0.8839683532714844\n",
      "Epoch 99, Loss: 0.6325554251670837\n",
      "Epoch 100, Loss: 0.9280614852905273\n",
      "Test Loss: 1.5559, Accuracy: 0.3500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.5559093952178955, 0.35)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming you have a Set Transformer model defined as 'model'\n",
    "model = SmallDeepSet()\n",
    "\n",
    "train_path = '/projectnb/ec500kb/projects/project7/set_transformer/model_weights_deep_set.pth'\n",
    "evaluate_path = '/projectnb/ec500kb/projects/project7/set_transformer/model_weights_deep_set.pth'\n",
    "\n",
    "train(model, train_loader, epochs=100, save_path=train_path)\n",
    "evaluate(model, test_loader, load_path=evaluate_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eeeed1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81855365",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
